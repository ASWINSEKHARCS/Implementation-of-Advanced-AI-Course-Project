{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *Project Title* :  **Comparison of U-Net architecture and Vision Transformer architecture for Ultrasound Image Reconstruction.**"
      ],
      "metadata": {
        "id": "K1itFYpmhAr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset\n",
        "\n",
        "**Training Data**: [Zenodo Dataset](https://zenodo.org/records/7813791)\n",
        "\n",
        "**Testing Data**:  [IEEE IUS Challenge Dataset](https://www.creatis.insa-lyon.fr/Challenge/IEEE_IUS_2016/home)\n",
        "\n",
        "Input_shape: 128* 374*128 (RF data)\n",
        "\n",
        "Image_shape: 374*128"
      ],
      "metadata": {
        "id": "Yiah0ohShNgx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COj6GyKSX6ae"
      },
      "source": [
        "# Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjtQ3NObQAs6"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "!pip install torchmetrics\n",
        "from torch.utils.data import Dataset\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.ndimage import zoom\n",
        "from torch.nn import MSELoss, L1Loss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import argparse\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import StepLR,ReduceLROnPlateau\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchmetrics import StructuralSimilarityIndexMeasure as SSIM\n",
        "!pip install torch torchvision transformers\n",
        "!pip install monai\n",
        "from monai.networks.layers import HilbertTransform\n",
        "import scipy.io\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU9IFB63Xn0q"
      },
      "source": [
        "#Read the data and store in the colab local space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvZWkXRuYaXw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvgRRH6f08Sp"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    @staticmethod\n",
        "    def getFiles(path):\n",
        "        paths = []\n",
        "        for root, _, files in os.walk(path):\n",
        "            for file in files:\n",
        "                paths.append(os.path.join(root, file))\n",
        "        return paths\n",
        "\n",
        "    @staticmethod\n",
        "    def readFile(path):\n",
        "        with h5py.File(path, \"r\") as dataFrame:\n",
        "            inp = np.array(dataFrame['inp'], dtype=\"float32\")  / 16384\n",
        "            out = np.array(dataFrame['out'], dtype=\"float32\")\n",
        "        return inp, out\n",
        "\n",
        "    def __init__(self, path):\n",
        "        super(CustomDataset, self).__init__()\n",
        "        self.filePaths = self.getFiles(path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filePaths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if index < len(self.filePaths):\n",
        "            path = self.filePaths[index]\n",
        "            inp, out = self.readFile(path)\n",
        "\n",
        "            inp = torch.from_numpy(inp).float()\n",
        "\n",
        "            out = torch.from_numpy(out).float()\n",
        "\n",
        "            return {\n",
        "                'input': inp,\n",
        "                'output': out\n",
        "            }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNet Architecure"
      ],
      "metadata": {
        "id": "LVLQJlhLpVgT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v6PPqQjN1A3w"
      },
      "outputs": [],
      "source": [
        "class AntiRectifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x - torch.mean(x, dim=1, keepdim=True)\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        pos_neg = torch.cat([F.relu(x), F.relu(-x)], dim=1)\n",
        "        return pos_neg\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        mid_channels = mid_channels or out_channels\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
        "        self.act1 = AntiRectifier()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(2 * mid_channels, out_channels // 2, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels // 2)\n",
        "        self.act2 = AntiRectifier()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.act2(x)\n",
        "        return x\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        x1 = F.interpolate(x1, size=x2.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "\n",
        "class UNetBeamformer(nn.Module):\n",
        "    def __init__(self, n_channels=128, bilinear=False):\n",
        "        super(UNetBeamformer, self).__init__()\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.up1 = Up(512, 256 , bilinear)\n",
        "        self.up2 = Up(256, 128, bilinear)\n",
        "        self.up3 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "\n",
        "        x = self.up1(x4, x3)\n",
        "        x = self.up2(x, x2)\n",
        "        x = self.up3(x, x1)\n",
        "        x = self.outc(x)\n",
        "\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "# # Example usage\n",
        "# # model = UNetBeamformer(n_channels=128)\n",
        "# # input_tensor = torch.randn(1, 128, 374, 128)  # batch size = 1 for example\n",
        "# # output = model(input_tensor)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer (Reconstruction task)"
      ],
      "metadata": {
        "id": "e6ZkB0k4DSFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ViTBeamformer(nn.Module):\n",
        "    def __init__(self, patch_dim=128, spatial_dim=(374, 128), embed_dim=512, num_heads=8, num_layers=6):\n",
        "        super(ViTBeamformer, self).__init__()\n",
        "        self.patch_dim = patch_dim\n",
        "        self.spatial_dim = spatial_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.flatten_spatial = spatial_dim[0] * spatial_dim[1]\n",
        "        self.patch_embedding = nn.Linear(self.flatten_spatial, embed_dim)\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(1, patch_dim, embed_dim))\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim * 4),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.output_projection = nn.Linear(embed_dim, self.flatten_spatial)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.contiguous().view(batch_size, self.patch_dim, -1)\n",
        "        x = self.patch_embedding(x)\n",
        "        x = x + self.positional_embedding\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.encoder(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.output_projection(x)\n",
        "        x = x.view(batch_size, self.patch_dim, *self.spatial_dim)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ZTQYAnWckumj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing, Training and Image Reconstruction"
      ],
      "metadata": {
        "id": "BOEG3lMejd82"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4G7TJJ81KXE"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "\n",
        "    @staticmethod\n",
        "    def norm(x):\n",
        "        return (x - torch.min(x)) / (torch.max(x) - torch.min(x))\n",
        "\n",
        "    def __init__(self, dataset, args, loss='MSE', split=0.8):\n",
        "        self.bs = args.bs\n",
        "        self.lr = args.lr\n",
        "        # self.model = UNetBeamformer()\n",
        "        self.model = ViTBeamformer()\n",
        "        self.criterion = L1Loss() if (loss == 'MAE') else MSELoss()\n",
        "\n",
        "\n",
        "        self.train_size = int(len(dataset) * split)\n",
        "        self.valid_size = len(dataset) - self.train_size\n",
        "        self.train_set, self.valid_set = random_split(\n",
        "            dataset, [self.train_size, self.valid_size], generator=torch.Generator().manual_seed(42)\n",
        "        )\n",
        "        self.train_data = DataLoader(self.train_set, batch_size=self.bs, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "        self.valid_data = DataLoader(self.valid_set, batch_size=self.bs, num_workers=args.num_workers, pin_memory=True)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.save_paths = args.save\n",
        "\n",
        "    def train(self, epochs, run_no):\n",
        "        save_paths = self.save_paths\n",
        "        os.makedirs(os.path.join(save_paths, \"chkpt/\", 'iter_' + str(run_no)), exist_ok=True)\n",
        "        os.makedirs(os.path.join(save_paths, \"logs/\"), exist_ok=True)\n",
        "        writer = SummaryWriter(os.path.join(save_paths, 'logs/iter_' + str(run_no)))\n",
        "\n",
        "        chkpt = os.path.join(save_paths, \"chkpt\", 'iter_' + str(run_no), \"model.pt\")\n",
        "        best = os.path.join(save_paths, \"chkpt\", 'iter_' + str(run_no), \"best.pt\")\n",
        "        optimizer = Adam(self.model.parameters(), lr=self.lr)\n",
        "        scaler = GradScaler()\n",
        "        #scheduler = StepLR(optimizer, step_size=100, gamma=0.5)  # Learning rate scheduler\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=15, factor=0.1, verbose=True)\n",
        "\n",
        "\n",
        "        if os.path.exists(chkpt):\n",
        "            checkpoint = torch.load(chkpt)\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            scheduler.load_state_dict(checkpoint.get('scheduler_state_dict', scheduler.state_dict()))\n",
        "            start = checkpoint['epoch']\n",
        "            train_step = checkpoint['train_step']\n",
        "            val_step = checkpoint['val_step']\n",
        "            threshold = checkpoint['loss']\n",
        "        else:\n",
        "            start = 0\n",
        "            train_step = 0\n",
        "            val_step = 0\n",
        "            threshold = 1000\n",
        "            ground_truth_logged = True\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(start, epochs):\n",
        "            self.model.train()\n",
        "            train_loss = 0\n",
        "            val_loss = 0\n",
        "\n",
        "            for i, batch in enumerate(self.train_data):\n",
        "                input_data = batch['input'].to(self.device)\n",
        "                output = batch['output'].to(self.device)\n",
        "                # print(\"Input_train: \", input_data.shape)\n",
        "                # print(\"output_train: \", output.shape)\n",
        "\n",
        "                with autocast():\n",
        "                    pred = self.model(input_data)\n",
        "                    # pred = pred.unsqueeze(1)\n",
        "                    # print(\"Pred_train: \", pred.shape)\n",
        "                    beamformed = torch.mul(pred, input_data)\n",
        "                    beamformed_sum = torch.sum(beamformed, axis=1)\n",
        "                    beamformed_sum = HilbertTransform(axis=1)(beamformed_sum)\n",
        "                    envelope = torch.abs(beamformed_sum)\n",
        "                    imPred = 20 * torch.log10(envelope / torch.clip(torch.max(envelope), min=1e-8))\n",
        "                    loss = self.criterion(imPred, output) / 4  # Gradient accumulation over 4 steps\n",
        "\n",
        "                # loss.backward()  # Backpropagation\n",
        "                # optimizer.step()  # Update the weights\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Accumulate gradients and update every 4 steps\n",
        "                if (i + 1) % 4 == 0:\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                train_loss += loss.item() * 4  # Undo the division for logging purposes\n",
        "\n",
        "                train_step += 1\n",
        "\n",
        "            train_loss /= len(self.train_set)\n",
        "\n",
        "            # Validation loop\n",
        "            val_loss, ground_truth_logged = self.validate(val_step, writer, epoch, ground_truth_logged)\n",
        "\n",
        "            print(f'Epoch = {epoch:3d}, Training Loss = {train_loss:.3f}, Validation Loss = {val_loss:.3f}')\n",
        "            writer.add_scalar('Train Loss', train_loss, global_step=epoch)\n",
        "            writer.add_scalar('Validation Loss', val_loss, global_step=epoch)\n",
        "            # Learning rate scheduler step\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # Save checkpoint\n",
        "            if epoch % 5 == 0 or val_loss < threshold:\n",
        "                print(\"Saving model checkpoint...\")\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict(),\n",
        "                    'loss': val_loss,\n",
        "                    'train_step': train_step,\n",
        "                    'val_step': val_step,\n",
        "                }, chkpt)\n",
        "\n",
        "            # Save the best model\n",
        "            if val_loss < threshold:\n",
        "                print(\"Saving best model weights\")\n",
        "                threshold = val_loss\n",
        "                torch.save(self.model.state_dict(), best)\n",
        "\n",
        "            writer.flush()\n",
        "\n",
        "        writer.close()\n",
        "\n",
        "    def validate(self, val_step, writer, epoch, ground_truth_logged):\n",
        "        self.model.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.valid_data:\n",
        "                input_data = batch['input'].to(self.device)\n",
        "                output = batch['output'].to(self.device)\n",
        "                # print(\"Input_val: \", input_data.shape)\n",
        "                # print(\"output_val: \", output.shape)\n",
        "\n",
        "                with autocast():\n",
        "                    pred = self.model(input_data)\n",
        "                    # pred = pred.unsqueeze(1)\n",
        "                    # print(\"Pred_val: \", pred.shape)\n",
        "                    beamformed = torch.mul(pred, input_data)\n",
        "                    beamformed_sum = torch.sum(beamformed, axis=1)\n",
        "                    beamformed_sum = HilbertTransform(axis=1)(beamformed_sum)\n",
        "                    envelope = torch.abs(beamformed_sum)\n",
        "                    imPred = 20 * torch.log10(envelope / torch.clip(torch.max(envelope), min=1e-8))\n",
        "                    loss = self.criterion(imPred, output)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_step += 1\n",
        "\n",
        "        val_loss /= len(self.valid_set)\n",
        "\n",
        "        # Now save the best image to see the iteration\n",
        "\n",
        "        # Convert to numpy for visualization\n",
        "        imPred = self.norm(imPred)\n",
        "        output = self.norm(output)\n",
        "        imPred = imPred.detach().cpu().numpy()\n",
        "        output = output.detach().cpu().numpy()\n",
        "        # TensorBoard Image logging\n",
        "        # Log ground truth only once, during the first batch\n",
        "        if ground_truth_logged:\n",
        "            writer.add_image('GT', output[0], global_step=epoch, dataformats='HW')\n",
        "        writer.add_image('Pred'+str(epoch), imPred[0], global_step=epoch, dataformats='HW')\n",
        "\n",
        "        # Visualization using matplotlib\n",
        "        # Display the ground truth and prediction for visual reference, only once\n",
        "        if ground_truth_logged:\n",
        "            plt.imshow(output[0], cmap='gray', aspect='auto')\n",
        "            plt.axis('off')\n",
        "            plt.title('Ground Truth')\n",
        "            plt.show()\n",
        "            ground_truth_logged = False  # Ensure ground truth is logged once\n",
        "\n",
        "        plt.imshow(imPred[0], cmap='gray', aspect='auto')\n",
        "        plt.axis('off')\n",
        "\n",
        "        return val_loss,ground_truth_logged\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMakU7Gc1Yxo"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--data\", type=str,default = '/content/drive/MyDrive/wrist_tofc')\n",
        "parser.add_argument(\"--save\", type=str,default = '/content/drive/MyDrive/Final_Results ViT_300Epoch_MAE')\n",
        "parser.add_argument(\"--epochs\", type=int,default=10)\n",
        "parser.add_argument(\"--bs\", type=int,default=8)\n",
        "parser.add_argument(\"--lr\", type=float,default=1e-3)\n",
        "parser.add_argument(\"--run\", type=int,default=1)\n",
        "parser.add_argument(\"--num_workers\", type=int,default=1)\n",
        "args = parser.parse_args([])  # This simulates no command-line arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmQuv4jZx4BF"
      },
      "outputs": [],
      "source": [
        "DATASET = CustomDataset('/content/drive/MyDrive/wrist_tofc')\n",
        "T = Trainer(DATASET, args, loss = 'MAE')\n",
        "T.train(epochs=300, run_no=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZI1-rqHocIq"
      },
      "source": [
        "Seeing the tensorboard logs for the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nCopt9jCzl1"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/Results ViT_ScratchModel_MSE_Error_30Epoch/logs\\iter_1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Model"
      ],
      "metadata": {
        "id": "ATJP0GEylUtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_path = \"/content/drive/MyDrive/ground_truth_image.mat\"\n",
        "test_data_path = \"/content/drive/MyDrive/Test_data.mat\"\n",
        "ground_truth = scipy.io.loadmat(ground_truth_path)\n",
        "test_data = scipy.io.loadmat(test_data_path)\n",
        "ground_truth_data = ground_truth['groundtruth_image']\n",
        "test_data_data = test_data['resampled_tof_corrected_data']\n",
        "ground_truth_tensor = torch.tensor(ground_truth_data, dtype=torch.float32)\n",
        "test_data_tensor = torch.tensor(test_data_data, dtype=torch.float32).unsqueeze(0)\n",
        "model_path = \"/content/drive/MyDrive/best_ViT.pt\"\n",
        "model = ViTBeamformer()\n",
        "# model = UNetBeamformer()\n",
        "# model.load_state_dict(torch.load(model_path))\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    model_output = model(test_data_tensor)\n",
        "    beamformed = torch.mul(model_output, test_data_tensor)\n",
        "    beamformed_sum = torch.sum(beamformed, dim=1)\n",
        "    envelope = torch.abs(beamformed_sum.squeeze(0))\n",
        "    imPred = 20 * torch.log10(envelope / torch.clamp(torch.max(envelope), min=1e-8))\n",
        "\n",
        "mse_loss_fn = MSELoss()\n",
        "mae_loss_fn = L1Loss()\n",
        "ssim_metric = SSIM(data_range=1.0)\n",
        "imPred_normalized = (imPred - imPred.min()) / (imPred.max() - imPred.min())\n",
        "ground_truth_normalized = (ground_truth_tensor - ground_truth_tensor.min()) / (\n",
        "    ground_truth_tensor.max() - ground_truth_tensor.min()\n",
        ")\n",
        "mse_loss = mse_loss_fn(imPred, ground_truth_tensor).item()\n",
        "mae_loss = mae_loss_fn(imPred, ground_truth_tensor).item()\n",
        "ssim_score = ssim_metric(\n",
        "    imPred_normalized.unsqueeze(0).unsqueeze(0),\n",
        "    ground_truth_normalized.unsqueeze(0).unsqueeze(0),\n",
        ").item()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(ground_truth_tensor.cpu().numpy(), cmap='gray')\n",
        "plt.title(\"Ground Truth\")\n",
        "plt.axis('off')\n",
        "plt.savefig('ground_truth_image.png')\n",
        "plt.show()\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(imPred.cpu().numpy(), cmap='gray')\n",
        "plt.title(\"Beamformed Prediction\")\n",
        "plt.axis('off')\n",
        "plt.savefig('beamformed_prediction.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UNkjG0oz1rUI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}